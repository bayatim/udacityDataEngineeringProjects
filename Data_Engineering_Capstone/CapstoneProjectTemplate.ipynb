{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "* The goal of this project is to evaluate the impact of weather's temperature on immagrants movements over April, 2016 in USA\n",
    "* Apache Spark is used to extract and transform raw data, and make a datawarehouse in parquet file format. \n",
    "* The star schema is used to develop a database, which will be effectively used for handling analytical queries.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "-- Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "* This project extracts raw data from two sources as described below. \n",
    "* It creates a datamodel of immagrants' movement in US consistsing of one fact tables referencing two dimension tables.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "-- Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "##### I94 Immigration Data: \n",
    "* This data comes from the [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "* Dataset includes infoes on individual incomming immigrants and thei ports on entry. \n",
    "* Data dictionary: utils/I94_SAS_Labels_Descriptions.SAS\n",
    "* Sample file: sample_data/immigration_data_sample.csv\n",
    "* Columns:\n",
    "    * 'i94yr': '4 digit year',\n",
    "    * 'i94mon': 'Numeric month',\n",
    "    * 'i94addr': 'where the immigrants resides in USA',\n",
    "    * 'i94port' : 'entry port of immagrant'\n",
    "\n",
    "##### World Temperature Data:\n",
    "* This dataset comes from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "* Dataset includes infoe on temprature on cities globally. This project only uses data of US cities.\n",
    "* Columns:\n",
    "    * 'dt': 'data of recorded temperature'\n",
    "    * 'AverageTemperature': 'the average temperature recorded over a month',\n",
    "    * 'City': 'city name',\n",
    "    * 'Country': 'country name',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# do all imports and installs here\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import pyspark.sql.dataframe as psd\n",
    "import pyspark.sql.session as pss\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.reader import Reader\n",
    "from utils.help_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:geting root paths to read and write data\n",
      "INFO:root:getting spark session\n",
      "INFO:root:stage raw data into parquet files if not already exist!!!\n",
      "INFO:root:i94 immigration raw data are are already written into parquet files\n",
      "INFO:root:world temperature raw data are are already written into parquet files\n",
      "INFO:root:Start fitting i94immigration data to reader class ...\n",
      "INFO:root:End fitting i94immigration data to reader class!\n",
      "INFO:root:Start fitting worldtemperature data to reader class ...\n",
      "INFO:root:End fitting worldtemperature data to reader class!\n"
     ]
    }
   ],
   "source": [
    "# initialize reader classs\n",
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = reader.spark\n",
    "immigration_df = reader.immigration_df\n",
    "temperature_df = reader.temperature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://166fe565ff1f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f60e6c8a5c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Wrangling: Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter needed columns\n",
    "immigration_usa_df = filter_immigration_data(immigration_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "immigration_usa_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>i94port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>HHW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>HHW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>HHW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>HOU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>LOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i94yr  i94mon i94addr i94port\n",
       "0  2016.0     4.0      CA     LOS\n",
       "1  2016.0     4.0      NV     LOS\n",
       "2  2016.0     4.0      WA     LOS\n",
       "3  2016.0     4.0      WA     LOS\n",
       "4  2016.0     4.0      WA     LOS\n",
       "5  2016.0     4.0      HI     HHW\n",
       "6  2016.0     4.0      HI     HHW\n",
       "7  2016.0     4.0      HI     HHW\n",
       "8  2016.0     4.0      FL     HOU\n",
       "9  2016.0     4.0      CA     LOS"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print immigration data \n",
    "immigration_usa_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create view for immigration data\n",
    "immigration_usa_df.createOrReplaceTempView('immigration_usa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count unique values in a given spark sql table\n",
    "def value_counts(table, col):\n",
    "    return reader.spark.sql(f\"\"\"\n",
    "    select {col}, count({col}) as count\n",
    "    from {table}\n",
    "    group by {col}\n",
    "    order by count\n",
    "    \"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "| i94yr|  count|\n",
      "+------+-------+\n",
      "|2016.0|3096313|\n",
      "+------+-------+\n",
      "\n",
      "+------+-------+\n",
      "|i94mon|  count|\n",
      "+------+-------+\n",
      "|   4.0|3096313|\n",
      "+------+-------+\n",
      "\n",
      "+-------+-----+\n",
      "|i94addr|count|\n",
      "+-------+-----+\n",
      "|   null|    0|\n",
      "|     KF|    1|\n",
      "|     52|    1|\n",
      "|     71|    1|\n",
      "|     S6|    1|\n",
      "|     85|    1|\n",
      "|     UL|    1|\n",
      "|     RU|    1|\n",
      "|     VL|    1|\n",
      "|     RA|    1|\n",
      "|     UR|    1|\n",
      "|     ZN|    1|\n",
      "|     TC|    1|\n",
      "|     PD|    1|\n",
      "|     YH|    1|\n",
      "|     EX|    1|\n",
      "|     RF|    1|\n",
      "|     RO|    1|\n",
      "|     73|    1|\n",
      "|     FC|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----+\n",
      "|i94port|count|\n",
      "+-------+-----+\n",
      "|    PHF|    1|\n",
      "|    NC8|    1|\n",
      "|    CNC|    1|\n",
      "|    VNB|    1|\n",
      "|    COO|    1|\n",
      "|    ERC|    1|\n",
      "|    CPX|    1|\n",
      "|    PCF|    1|\n",
      "|    LWT|    1|\n",
      "|    NIG|    1|\n",
      "|    MAI|    1|\n",
      "|    RIO|    1|\n",
      "|    HNN|    1|\n",
      "|    YIP|    1|\n",
      "|    ANA|    1|\n",
      "|    SCH|    1|\n",
      "|    BWM|    1|\n",
      "|    REN|    1|\n",
      "|    MND|    1|\n",
      "|    BHX|    2|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count unique values of each column\n",
    "value_counts('immigration_usa_table', 'i94yr')\n",
    "value_counts('immigration_usa_table', 'i94mon')\n",
    "value_counts('immigration_usa_table', 'i94addr')\n",
    "value_counts('immigration_usa_table', 'i94port')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The above descriptive results show that both 'i94yr' and 'i94mon' columns are clean, but there are lots of invalid codes in 'i94addr' and 'i94port' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We assumed that all immigrants came to the US on the first day of a given month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter out valid ports\n",
    "validPorts = get_valid_ports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean immigration data: remove null values, invalid ports and cities\n",
    "clean_immigration_usa_df = clean_immigration_data(validPorts, immigration_usa_df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "clean_immigration_usa_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a view of clean immigration data\n",
    "clean_immigration_usa_df.createOrReplaceTempView('clean_immigration_usa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|i94addr|count|\n",
      "+-------+-----+\n",
      "|     99|   52|\n",
      "|     VI|  226|\n",
      "|     WY|  460|\n",
      "|     SD|  557|\n",
      "|     WV|  808|\n",
      "|     ND| 1225|\n",
      "|     MT| 1339|\n",
      "|     VT| 1477|\n",
      "|     AK| 1604|\n",
      "|     ID| 1752|\n",
      "|     MS| 1771|\n",
      "|     NM| 1994|\n",
      "|     ME| 2361|\n",
      "|     NH| 2817|\n",
      "|     AR| 2873|\n",
      "|     DE| 3111|\n",
      "|     KS| 3224|\n",
      "|     OK| 3239|\n",
      "|     RI| 3289|\n",
      "|     IA| 3391|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----+\n",
      "|i94port|count|\n",
      "+-------+-----+\n",
      "|    FRI|    1|\n",
      "|    VNB|    1|\n",
      "|    HNN|    1|\n",
      "|    NC8|    1|\n",
      "|    SPO|    1|\n",
      "|    RIO|    1|\n",
      "|    CPX|    1|\n",
      "|    MGM|    1|\n",
      "|    BWM|    1|\n",
      "|    NIG|    1|\n",
      "|    ANA|    1|\n",
      "|    LWT|    1|\n",
      "|    PHF|    1|\n",
      "|    YIP|    1|\n",
      "|    MND|    1|\n",
      "|    PSM|    2|\n",
      "|    ADW|    2|\n",
      "|    NOO|    2|\n",
      "|    SGJ|    2|\n",
      "|    MTH|    2|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if invalid data are cleaned out\n",
    "value_counts('clean_immigration_usa_table', 'i94addr')\n",
    "value_counts('clean_immigration_usa_table', 'i94port')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Wrangling: Temperature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We only focus on data from usa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* As the immigration data is from 2016, we only select temperature data as close to this year as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select needed columns\n",
    "temperature_usa_df = filter_temperature_data(temperature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        dt|count|\n",
      "+----------+-----+\n",
      "|2013-04-01|  257|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if only data for April, 2013 are selected\n",
    "temperature_usa_df.groupBy('dt').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add intry ports infos to temperature_usa_df (extracted from immigration data based on a given city)\n",
    "validPorts = get_valid_ports()\n",
    "temperature_usa_df = add_port_to_temperature_data(temperature_usa_df, validPorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "temperature_usa_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>i94port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>15.752999999999998</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Akron</td>\n",
       "      <td>9.691</td>\n",
       "      <td>AKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>11.555</td>\n",
       "      <td>ABQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>12.425</td>\n",
       "      <td>AXB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Allentown</td>\n",
       "      <td>9.722999999999999</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amarillo</td>\n",
       "      <td>12.954</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>15.380999999999998</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>-6.421</td>\n",
       "      <td>ANC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ann Arbor</td>\n",
       "      <td>6.819</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>United States</td>\n",
       "      <td>Antioch</td>\n",
       "      <td>15.995999999999999</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt        Country         City  AverageTemperature i94port\n",
       "0  2013-04-01  United States      Abilene  15.752999999999998    None\n",
       "1  2013-04-01  United States        Akron               9.691     AKR\n",
       "2  2013-04-01  United States  Albuquerque              11.555     ABQ\n",
       "3  2013-04-01  United States   Alexandria              12.425     AXB\n",
       "4  2013-04-01  United States    Allentown   9.722999999999999    None\n",
       "5  2013-04-01  United States     Amarillo              12.954    None\n",
       "6  2013-04-01  United States      Anaheim  15.380999999999998    None\n",
       "7  2013-04-01  United States    Anchorage              -6.421     ANC\n",
       "8  2013-04-01  United States    Ann Arbor               6.819    None\n",
       "9  2013-04-01  United States      Antioch  15.995999999999999    None"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize temperature data\n",
    "temperature_usa_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|AverageTemperature|\n",
      "+-------+------------------+\n",
      "|  count|               257|\n",
      "|   mean| 13.75025680933851|\n",
      "| stddev| 5.253205757503348|\n",
      "|    min|            -0.591|\n",
      "|    max| 9.722999999999999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deacribing Average Temperature data column\n",
    "temperature_usa_df.select(['AverageTemperature']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create temporal view of temperature_usa data\n",
    "temperature_usa_df.createOrReplaceTempView('temperature_usa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        dt|count|\n",
      "+----------+-----+\n",
      "|2013-04-01|  257|\n",
      "+----------+-----+\n",
      "\n",
      "+-------------+-----+\n",
      "|      Country|count|\n",
      "+-------------+-----+\n",
      "|United States|  257|\n",
      "+-------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|           City|count|\n",
      "+---------------+-----+\n",
      "|    Chattanooga|    1|\n",
      "|      Worcester|    1|\n",
      "|     Charleston|    1|\n",
      "|          Tempe|    1|\n",
      "|         Corona|    1|\n",
      "|       Thornton|    1|\n",
      "|North Las Vegas|    1|\n",
      "|        Phoenix|    1|\n",
      "|      Hollywood|    1|\n",
      "|       Savannah|    1|\n",
      "|     Toms River|    1|\n",
      "| Pembroke Pines|    1|\n",
      "|  Coral Springs|    1|\n",
      "|          Omaha|    1|\n",
      "|      Anchorage|    1|\n",
      "|       Paradise|    1|\n",
      "|      Allentown|    1|\n",
      "|   Fort Collins|    1|\n",
      "|        Anaheim|    1|\n",
      "|     Greensboro|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+-----+\n",
      "|AverageTemperature|count|\n",
      "+------------------+-----+\n",
      "|             17.16|    1|\n",
      "|            21.193|    1|\n",
      "|             2.543|    1|\n",
      "|15.752999999999998|    1|\n",
      "|            13.546|    1|\n",
      "|             9.264|    1|\n",
      "|            18.971|    1|\n",
      "|            18.677|    1|\n",
      "|            14.939|    1|\n",
      "|            10.119|    1|\n",
      "|13.040999999999999|    1|\n",
      "| 6.652000000000001|    1|\n",
      "|             19.09|    1|\n",
      "|            18.158|    1|\n",
      "|              6.93|    1|\n",
      "|            13.341|    1|\n",
      "|            -0.591|    1|\n",
      "|16.741999999999994|    1|\n",
      "|20.666999999999998|    1|\n",
      "|            12.308|    1|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----+\n",
      "|i94port|count|\n",
      "+-------+-----+\n",
      "|   null|    0|\n",
      "|    BUR|    1|\n",
      "|    SNA|    1|\n",
      "|    GRB|    1|\n",
      "|    RIV|    1|\n",
      "|    OAK|    1|\n",
      "|    DET|    1|\n",
      "|    RFD|    1|\n",
      "|    CID|    1|\n",
      "|    TAC|    1|\n",
      "|    OTM|    1|\n",
      "|    NSV|    1|\n",
      "|    SFR|    1|\n",
      "|    AFW|    1|\n",
      "|    JER|    1|\n",
      "|    BHX|    1|\n",
      "|    LLB|    1|\n",
      "|    SAV|    1|\n",
      "|    HAR|    1|\n",
      "|    NOG|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count unique values in each column\n",
    "for col in temperature_usa_df.columns: value_counts('temperature_usa_table', col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Adding i94port column to temprature dataframe. It is mapped from cleaned up immigration dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "validPorts = get_valid_ports()\n",
    "temperature_usa_df = add_port_to_temperature_data(temperature_usa_df, validPorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_usa_df.createOrReplaceTempView('temperature_usa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_temerature_usa_df = clean_temerature_usa_data(temperature_usa_df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+------------------+-------+\n",
      "|        dt|      Country|       City|AverageTemperature|i94port|\n",
      "+----------+-------------+-----------+------------------+-------+\n",
      "|2013-04-01|United States|      Akron|              9.69|    AKR|\n",
      "|2013-04-01|United States|Albuquerque|             11.56|    ABQ|\n",
      "|2013-04-01|United States| Alexandria|             12.43|    AXB|\n",
      "|2013-04-01|United States|  Anchorage|             -6.42|    ANC|\n",
      "|2013-04-01|United States|    Atlanta|             14.53|    ATL|\n",
      "+----------+-------------+-----------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_temerature_usa_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "* **Data model**:\n",
    "    * The star schema is used as datamodel of this project.\n",
    "    * It is a relational model contains one fact table named fact_table surrounded by two dimension tables named dim_immigration_table and dim_temperature_table.\n",
    "    * It suits analytical queries and user can analyze the data with few number of joins.\n",
    "\n",
    "\n",
    "* **Fact table** - dim_immigration table joined with the dim_temperature table on i94port and dt, Columns:\n",
    "    * dt - date of arrival,\n",
    "    * i94port: 3 character code of destination USA city,\n",
    "    * AverageTemperature: average temperature of destination city\n",
    "\n",
    "\n",
    "* **Immigration dimension table**:\n",
    "    * dt - date of arrival,\n",
    "    * i194addr: where the immigrants resides in USA (2 character code),\n",
    "    * i94port: 3 character code of destination USA city\n",
    "\n",
    "\n",
    "* **Temperature dimension table**:\n",
    "    * dt: date of recorded temperature\n",
    "    * AverageTemperature: average temperature\n",
    "    * City: city name\n",
    "    * Country: country name\n",
    "    * i94port: 3 character code of entry port (extracted from i94-immigration data)\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "* Import all neccassery libraries and functions\n",
    "* Initialize reader object and assign spark and data to appropriate variables\n",
    "* Process immigration data to create clean immigration dimention table\n",
    "    * filter, clean and load data\n",
    "* Process temperature data to create clean temperature dimention table\n",
    "    * filter, clean and load data\n",
    "* Preate fact table from dimention tables\n",
    "    * join dimention tables and load data\n",
    "* Perform data quality check\n",
    "    * Count check to ensure completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# do all imports and installs here\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import pyspark.sql.dataframe as psd\n",
    "import pyspark.sql.session as pss\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.reader import Reader\n",
    "from utils.help_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:geting root paths to read and write data\n",
      "INFO:root:getting spark session\n",
      "INFO:root:stage raw data into parquet files if not already exist!!!\n",
      "INFO:root:i94 immigration raw data are are already written into parquet files\n",
      "INFO:root:world temperature raw data are are already written into parquet files\n",
      "INFO:root:Start fitting i94immigration data to reader class ...\n",
      "INFO:root:End fitting i94immigration data to reader class!\n",
      "INFO:root:Start fitting worldtemperature data to reader class ...\n",
      "INFO:root:End fitting worldtemperature data to reader class!\n"
     ]
    }
   ],
   "source": [
    "# initialize reader classs\n",
    "reader = Reader()\n",
    "\n",
    "# assign spark, immigration data, and temperature data to corresponding valiables\n",
    "spark = reader.spark\n",
    "immigration_df = reader.immigration_df\n",
    "temperature_df = reader.temperature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data(immigration_df: psd.DataFrame, spark: pss.SparkSession):\n",
    "    # extract columns to create immigration_usa table\n",
    "    immigration_usa_df = filter_immigration_data(immigration_df)\n",
    "\n",
    "    # create temporal view of immigration_usa data\n",
    "    immigration_usa_df.createOrReplaceTempView('immigration_usa_table')\n",
    "\n",
    "    # remove null values from immigration_usa columns\n",
    "    validPorts = get_valid_ports()\n",
    "    clean_immigration_usa_df = clean_immigration_data(validPorts, immigration_usa_df, spark)\n",
    "\n",
    "    # write clean immigration_usa table to parquet files\n",
    "    print(\"Start writting immigration_usa table to output path ....\")\n",
    "    clean_immigration_usa_df.write.mode(\"overwrite\").partitionBy(\"i94port\").parquet(\"output_data/immigration.parquet\")\n",
    "    print(\"End writting immigration_usa table to output path!\")\n",
    "    \n",
    "    # create a view of clean immigration_usa data for further analysis\n",
    "    clean_immigration_usa_df.createOrReplaceTempView('clean_immigration_usa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_temperature_data(temperature_df: psd.DataFrame, spark: pss.SparkSession):\n",
    "    # extract columns to create temperature_usa table\n",
    "    temperature_usa_df = filter_temperature_data(temperature_df)\n",
    "    \n",
    "    # add intry ports infos to temperature_usa_df\n",
    "    validPorts = get_valid_ports()\n",
    "    temperature_usa_df = add_port_to_temperature_data(temperature_usa_df, validPorts)\n",
    "    \n",
    "    # create temporal view of temperature_usa data\n",
    "    temperature_usa_df.createOrReplaceTempView('temperature_usa_table')\n",
    "\n",
    "    # remove null values from temperature_usa columns\n",
    "    clean_temperature_usa_df = clean_temerature_usa_data(temperature_usa_df, spark)\n",
    "\n",
    "    # write clean temperature_usa table to parquet files\n",
    "    print(\"Start writting temperature_usa table to output path ....\")\n",
    "    clean_temperature_usa_df.write.mode(\"overwrite\").partitionBy(\"i94port\").parquet(\"output_data/temprature.parquet\")\n",
    "    print(\"End writting temperature_usa table to output path!\")\n",
    "    \n",
    "    # create a view of clean temperature_usa data for further analysis\n",
    "    clean_temperature_usa_df.createOrReplaceTempView('clean_temperature_usa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_fact_table(spark: pss.SparkSession):\n",
    "    # extract columns to create fact table by joining clean_temperature_usa_table and clean_immigration_usa_table\n",
    "    fact_df = spark.sql(query_create_fact_table())\n",
    "    \n",
    "    # write clean fact table to parquet files\n",
    "    print(\"Start writting fact table to output path ....\")\n",
    "    fact_df.write.mode(\"overwrite\").partitionBy(\"i94port\").parquet(\"output_data/fact.parquet\")\n",
    "    print(\"End writting fact table to output path!\")\n",
    "    \n",
    "    # create a view of fact data for further analysis\n",
    "    fact_df.createOrReplaceTempView('fact_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def immigration_movement_etl():\n",
    "    # proccess immigration data to create immigration dimention table\n",
    "    process_immigration_data(immigration_df, spark)\n",
    "    \n",
    "    # proccess world tempetature data to create temperature dimention table\n",
    "    process_temperature_data(temperature_df, spark)\n",
    "    \n",
    "    # proccess created dimention tables to create fact table\n",
    "    process_fact_table(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writting immigration_usa table to output path ....\n",
      "End writting immigration_usa table to output path!\n",
      "Start writting temperature_usa table to output path ....\n",
      "End writting temperature_usa table to output path!\n",
      "Start writting fact table to output path ....\n",
      "End writting fact table to output path!\n"
     ]
    }
   ],
   "source": [
    "# run ETL pipeline \n",
    "immigration_movement_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "* Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "    * Check the number of netries to ensure completeness\n",
    "  \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running data quality check on table clean_immigration_usa_table\n",
      "INFO:root:Getting number of entries in table clean_immigration_usa_table\n",
      "INFO:root:Table clean_immigration_usa_table has [2917199] numbers of entries.\n",
      "INFO:root:Data quality check passed for table clean_immigration_usa_table\n",
      "INFO:root:Running data quality check on table clean_temperature_usa_table\n",
      "INFO:root:Getting number of entries in table clean_temperature_usa_table\n",
      "INFO:root:Table clean_temperature_usa_table has [117] numbers of entries.\n",
      "INFO:root:Data quality check passed for table clean_temperature_usa_table\n",
      "INFO:root:Running data quality check on table fact_table\n",
      "INFO:root:Getting number of entries in table fact_table\n",
      "INFO:root:Table fact_table has [2917199] numbers of entries.\n",
      "INFO:root:Data quality check passed for table fact_table\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks\n",
    "check_exist_rows(['clean_immigration_usa_table', 'clean_temperature_usa_table', 'fact_table'], spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running data integrity check on fact and dimmention tables\n",
      "INFO:root:Getting number of entries in table clean_immigration_usa_table\n",
      "INFO:root:Table clean_immigration_usa_table has [2917199] numbers of entries.\n",
      "INFO:root:Getting number of entries in table fact_table\n",
      "INFO:root:Table fact_table has [2917199] numbers of entries.\n",
      "INFO:root:Data integrity check passed for tables fact_table and clean_immigration_usa_table\n"
     ]
    }
   ],
   "source": [
    "# check if the number of entries in fact table is same as clean_immigration_usa_table\n",
    "check_tables_integrity('fact_table', 'clean_immigration_usa_table', spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running data constrain check on fact table\n",
      "INFO:root:Getting number of null values in table fact_table\n",
      "INFO:root:Constrain integrity check passed for tables fact_table\n"
     ]
    }
   ],
   "source": [
    "# check if null values are not existed in fact table for dt and i94port columns\n",
    "check_constrain_integrity('fact_table', spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "Included in **utils/DataDictionary.md**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* In this project I used Apache Spark to do all of the ETL data processing. It is capable of scaling a large amount of data sets and has very useful functions for cleaning up and wangling data. In this project, we only examined data recorded over a month. We can easily expand the project scope and model several months of data because the use of the parquet format can easily be scaled to a much larger dataset. If we choose to examine data on a monthly basis, Apache Airflow can be used to partition data by month and analyze them separately.\n",
    "\n",
    "* I would approach the problem as follows under different conditions:\n",
    "\n",
    "    * If the data has increased by 100x: Spark can still process it, one may need to use more cluster nodes. I would also consider using the Redshift Analytical database as it is optimized for aggregation and performs very well on heavy workloads. Based on the size of our dataset, we can adjust the size of the EMR cluster.\n",
    "\n",
    "    * If my job was to update data on a daily basis, I would definitely use Apache Airflow to create a schedule for the update.\n",
    "\n",
    "    * If more than 100 people need to access the data, more CPU resources are needed. By using a distributed database like Redshift database we can improve our replications and partitioning, so that users can get faster query results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
