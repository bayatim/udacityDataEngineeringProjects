# udacityDataEngineeringProjects

## The Project Journey
The projects will take us on a journey where we’ll assume the role of a Data Engineer at a fabricated data streaming company called “Sparkify” as it scales its data engineering in both size and sophistication. We’ll work with simulated data of listening behavior, as well as a wealth of metadata related to songs and artists. We’ll start working with a small amount of data, with low complexity, processed and stored on a single machine. By the end, we’ll develop a sophisticated set of data pipelines to work with massive amounts of data processed and stored on the cloud. There are five projects in the program. Below is a description of each.

## Project 1 - Data Modeling with Postgres
In this project, we’ll model user activity data for a music streaming app called Sparkify. We’ll create a database and import data stored in CSV and JSON files, and model the data. We’ll do this with a relational model in Postgres. We’ll design the data models to optimize queries for understanding what songs users are listening to. For PostgreSQL, we will also define Fact and Dimension tables and insert data into your new tables.

Link : [Data_Modeling_with_Postgres](https://github.com/bayatim/udacityDataEngineeringProjects/tree/main/Data_Modeling_with_Postgres)

#TODO LInk
## Project 2 - Data Modeling with Apache Cassandra
In this project, we’ll model user activity data for a music streaming app called Sparkify. We’ll create a database and import data stored in CSV and JSON files, and model the data. You’ll do this with a NoSQL data model with Apache Cassandra. We’ll design the data models to optimize queries for understanding what songs users are listening to. For Apache Cassandra, we will model our data to help the data team at Sparkify answer queries about app usage. We will set up your Apache Cassandra database tables in ways to optimize writes of transactional data on user sessions.

Link : [Data_Modeling_with_Apache_Cassandra](https://github.com/bayatim/udacityDataEngineeringProjects/tree/main/Data_Modeling_with_Apache_Cassandra)

## Project 3 - Cloud Data Warehousing
In this project, we’ll move to the cloud as you work with larger amounts of data. We are tasked with building an ELT pipeline that extracts Sparkify’s data from S3, Amazon’s popular storage system. From there, we’ll stage the data in Amazon Redshift and transform it into a set of fact and dimensional tables for the Sparkify analytics team to continue finding insights in what songs their users are listening to.

Link : [Cloud_Data_Warehousing](https://github.com/bayatim/udacityDataEngineeringProjects/tree/main/Cloud_Data_Warehousing)

## Project 4 - Data Lakes with Apache Spark
In this project, we'll build an ETL pipeline for a data lake. The data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in the app. We will load data from S3, process the data into analytics tables using Spark, and load them back into S3. We'll deploy this Spark process on a cluster using AWS.

Link : [Data_Lakes_with_Apache_Spark](https://github.com/bayatim/udacityDataEngineeringProjects/tree/main/Data_Lakes_with_Apache_Spark)

## Project 5 - Data Pipelines with Apache Airflow
In this project, we’ll continue your work on Sparkify’s data infrastructure by creating and automating a set of data pipelines. We’ll use the up-and-coming tool Apache Airflow, developed and open-sourced by Airbnb and the Apache Foundation. We’ll configure and schedule data pipelines with Airflow, setting dependencies, triggers, and quality checks as you would in a production setting.

Link : [Data_Pipelines_with_Apache_Airflow](https://github.com/bayatim/udacityDataEngineeringProjects/tree/main/Data_Pipelines_with_Apache_Airflow)

## Project 6 - Data Engineering Capstone
The goal of this project is to evaluate the impact of weather's temperature on immagrants movements over April, 2016 in USA. Apache Spark is used to extract and transform raw data, and make a datawarehouse in parquet file format. The star schema is used to develop a database, which will be effectively used for handling analytical queries.

Link : [Data_Engineering_Capstone](https://github.com/bayatim/udacityDataEngineeringProjects/tree/main/Data_Engineering_Capstone)

